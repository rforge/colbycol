<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>R Package ColbyCol</title>
	<link href="http://r-forge.r-project.org/themes/rforge/styles/estilo1.css" rel="stylesheet" type="text/css" />
</head>

<body>

<!-- R-Forge Logo -->
<table border="0" width="100%" cellspacing="0" cellpadding="0">
<tr><td>
<a href="/"><img src="http://r-forge.r-project.org/themes/rforge/imagesrf/logo.png" border="0" alt="R-Forge Logo" /> </a> </td> </tr>
</table>


<h2>ColByCol R package</h2>

<p>This package is intended for reading big datasets into R. It can be useful under certain conditions.</p>

<h3>Reading big datasets into R</h3>

<p>By default, R reads data into memory. It creates some problems when dealing with large datasets. Many solutions have been proposed for dealing with such datasets. Some of them can be found <a href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">here</a>. Uploading your data into a DBMS can also be a solution.</p>

<p>However, read.table function remains the main data import function in R. This function is memory inefficient and, according to some estimates, it requires three times as much memory as the size of a dataset in order to read it into R.</p>

<p>The reason for such inefficiency is that R stores data.frames in memory as columns (a data.frame is no more than a list of equal length vectors) whereas text files consist of rows of records. Therefore, R's read.table needs to read whole lines, process them individually breaking into tokens and transposing these tokens into column oriented data structures.</p>

<h3>Short tutorial</h3>

First, we will create a <em>huge</em> file on disk. 

<pre>
n.rows    <- 10000
n.cols    <- 100
n.levels  <- 200

df.double <- data.frame( matrix( rnorm( n.rows * n.cols ), n.rows, n.cols) )
df.integ  <- data.frame( matrix( sample( 1:(n.rows * n.cols), n.rows * n.cols, replace = T ), n.rows, n.cols ) )


df.txt    <- sample( 1:20, n.levels, replace = T )
df.txt    <- sapply( df.txt, function(i) paste( sample( letters, i, replace = T ), collapse = "" ) )
df.txt    <- replicate( n.cols, sample( df.txt, n.rows, replace = T ), simplify = F )
df.txt    <- as.data.frame( df.txt )
colnames( df.txt ) <- paste( "txt", 1:n.cols, sep = "." )

df.all    <- cbind( df.double, df.integ, df.txt )

write.table( df.all, file = "bigsize.txt", col.names = T, row.names = F, sep = "," )
replicate( 25, write.table( df.all, file = "bigsize.txt", col.names = F, row.names = F, sep = ",", append = T ) )

</pre>

The file thus created is about 1GB in size. You can gauge the <code>n.rows</code>, <code>n.cols</code>, and <code>n.levels</code> to get file sizes suitable to your system.

If you are breve enough, you can then try to load the file in the <em>standard</em> way, i.e,

<pre>
can.i   <- read.table( "bigsize.txt", header = T, sep = "," )         # in fact, I cannot
</pre>

but I advise you not to.
Instead, you can use the <code>colbycol</code> package as follows:

<pre>
library( colbycol )
i.can <- cbc.read.table( "bigsize.txt", header = T, sep = "," )
</pre>

After a few minutes, your <code>i.can</code> object of class <code>colbycol</code> will be available. 
What you do after that with it is up to you, but you can try, for instance,

<pre>
summary( i.can )
colnames( i.can )

sapply( 1:100, function( x ) summary( cbc.get.col( i.can, x ) )  )

# my.df <- as.data.frame( i.can )                   # perhaps too much

my.df <- as.data.frame( i.can, columns = 1:10 )
my.df <- as.data.frame( i.can, columns = 1:10 )
my.df <- as.data.frame( i.can, columns = 1:10, rows = 1:300 )
my.df <- as.data.frame( i.can, rows = 1:300 )
</pre>

It is not even necessary to preprocess all the rows and columns from the original text file. For instance, the commands

<pre>
i.can <- cbc.read.table( "bigsize.txt", just.read = 1:100, header = T, sep = "," )
i.can <- cbc.read.table( "bigsize.txt", sample.pct = 0.5, header = T, sep = "," )
</pre>

will just read the columns <code>1:100</code> of the text file and a random sample of approximately 50% of its rows respectively.

<h3>ColByCol approach</h3>

<p>ColByCol approach is memory efficient. Using Java code, tt reads the input text file and outputs it into several text files, each holding an individual column of the original dataset. Then, these files are read individually into R thus avoiding R's memory bottleneck.</p>

<p>The approach works best for big files divided into many columns, specially when these columns can be transformed into memory efficient types and data structures: R representation of numbers (in some cases), and character vectors with repeated levels via factors occupy much less space than their character representation.</p>

<p>Package ColByCol has been successfully used to read multi-GB datasets on a 2GB laptop.</p>

<h3>Further package info</h3>

<p>You can find it in <a href="http://cran.r-project.org/web/packages/colbycol/index.html">CRAN</a> and can be installed using R's install.package function.</p>

<p>The project summary page can be found <a href="http://r-forge.r-project.org/projects/colbycol/">here</a>. You may also be interested in its page at <a href="http://crantastic.org/packages/colbycol">crantastic</a>.</p>

<p>The package has been developed by <a href="http://www.datanalytics.com">Carlos J. Gil Bellosta</a>.</p>
</body>
</html>

